{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%run Grid_world.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_ENOUGH = 1e-3 # threshold for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_values(V, g):\n",
    "  for i in range(g.rows):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.cols):\n",
    "      v = V.get((i,j), 0)\n",
    "      if v >= 0:\n",
    "        print(\" %.2f|\" % v, end=\"\")\n",
    "      else:\n",
    "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def print_policy(P, g):\n",
    "  for i in range(g.rows):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.cols):\n",
    "      a = P.get((i,j), ' ')\n",
    "      print(\"  %s  |\" % a, end=\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(states, grid, gamma):\n",
    "    V = {}\n",
    "    #Initialize zero value for all states\n",
    "    for state in states:\n",
    "        V[state] = 0.0\n",
    "        \n",
    "    run_nbr = 0\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        run_nbr += 1\n",
    "        #print(\"Run:\",run_nbr)\n",
    "        \n",
    "        for state in states:\n",
    "            #print(\"Updating for state\", state)\n",
    "            old_v = V[state]\n",
    "            #Make move only if its a valid one\n",
    "            if state in grid.actions:\n",
    "                grid.set_state(state)\n",
    "                v_new = 0.0\n",
    "                \n",
    "                # Find the new value of this state\n",
    "                p = 1.0/len(grid.actions[state])   #Assign equal probabilty to all actions\n",
    "                for action in grid.actions[state]:\n",
    "                    grid.set_state(state)\n",
    "                    reward = grid.move(action)\n",
    "                    v_new += p*(reward + gamma*V[grid.current_state()])  #Bellman Eq.\n",
    "                \n",
    "                V[state] = v_new\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[state]))\n",
    "        \n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            print(\"Random policy converged in run\", run_nbr)\n",
    "            break\n",
    "    \n",
    "    print(\"values for uniformly random actions:\")\n",
    "    print_values(V, grid)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_policy(states, grid, gamma, policy):\n",
    "    \n",
    "    print(\"Policy Grid:\")\n",
    "    print_policy(policy, grid)\n",
    "    \n",
    "    V = {}\n",
    "    #Initialize zero value for all states\n",
    "    for state in states:\n",
    "        V[state] = 0\n",
    "        \n",
    "    run_nbr = 0\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        run_nbr += 1\n",
    "        #print(\"Run:\",run_nbr)\n",
    "        \n",
    "        for state in states:\n",
    "            #print(\"Updating for state\", state)\n",
    "            old_v = V[state]\n",
    "            #Make move only if its a valid one\n",
    "            if state in policy:\n",
    "                grid.set_state(state)\n",
    "                v_new = 0.0\n",
    "                \n",
    "                # Find the new value of this state\n",
    "                p = 1.0   #Each state has only 1 action possible\n",
    "                for action in policy[state]:\n",
    "                    grid.set_state(state)\n",
    "                    reward = grid.move(action)\n",
    "                    v_new += p*(reward + gamma*V[grid.current_state()])  #Bellman Eq.\n",
    "                \n",
    "                V[state] = v_new\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[state]))\n",
    "        \n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            print(\"\\nFixed Policy converged in run\", run_nbr)\n",
    "            break\n",
    "    \n",
    "    print(\"Values for Fixed Policy actions:\")\n",
    "    print_values(V, grid)\n",
    "    print(\"\\n\")\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random policy converged in run 10\n",
      "values for uniformly random actions:\n",
      "---------------------------\n",
      " 0.06| 0.15| 0.27| 0.00|\n",
      "---------------------------\n",
      "-0.02| 0.00|-0.37| 0.00|\n",
      "---------------------------\n",
      "-0.11|-0.22|-0.38|-0.67|\n",
      "\n",
      "\n",
      "Policy Grid:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n",
      "\n",
      "Fixed Policy converged in run 4\n",
      "Values for Fixed Policy actions:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # iterative policy evaluation\n",
    "    # given a policy, let's find it's value function V(s)\n",
    "    # we will do this for both a uniform random policy and fixed policy\n",
    "    # NOTE:\n",
    "    # there are 2 sources of randomness\n",
    "    # p(a|s) - deciding what action to take given the state\n",
    "    # p(s',r|s,a) - the next state and reward given your action-state pair\n",
    "    # we are only modeling p(a|s) = uniform\n",
    "    # how would the code change if p(s',r|s,a) is not deterministic?\n",
    "\n",
    "    biggest_change = 0\n",
    "    \n",
    "    grid = standard_grid()\n",
    "    states = grid.all_states()\n",
    "    gamma = 0.9\n",
    "    random_policy(states, grid, gamma)\n",
    "    \n",
    "    policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "              }\n",
    "    \n",
    "    V = fixed_policy(states, grid, gamma, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
